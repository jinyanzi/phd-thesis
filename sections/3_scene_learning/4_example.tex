\section{Semantic knowledge visualization}

For topic model training, we make each video clip 90 frames ($\sim$ 3 sec); each frame is divided into $10\times10$ pixel grids. Different from $D=4$ in \cite{wang2009unsupervised,kuettel2010s}, we make $D=12$. This number considers motions more than horizontal and vertical, making subsequent post-processing more accurate. 
We extend a C++ implementation of HDP \footnote{Chong Wang, David Blei: https://github.com/blei-lab/hdp.} and train the models on IDOT dataset \cite{yanziVehicleTracker}, each video is around 5 minutes. 
In the following, we first give some examples of the scene learning results, then see how the semantic knowledge helps improve object tracking.

Complementing the partial results in previous sections, \ref{fig:entry-exit-full-1}, \ref{fig:entry-exit-full-2} and \ref{fig:entry-exit-full-3} provide complete results on several scenes, 
with the same representation as previous sections.
\ref{fig:entry-exit-full-1} summarizes a low-resolution video, where the four main motions are captured, however, some turning motions may be missed. This is likely because optical flow with small magnitude is filtered as error before it is fed to the topic model, or because such motions are rare or do not appear in the training video.
For higher resolution videos, such as \ref{fig:entry-exit-full-2} and \ref{fig:entry-exit-full-3}, optical flow results are more accurate and movement magnitudes are greater, when measured in pixels. Consequently, HDP can catch most visible motions, and even distinguish individual lanes.
By visual inspection, the obtained movements, as well as the locations and directions of entry/exit points, are consistent with the subject scenes. 

\input{img/scene_example.tex}